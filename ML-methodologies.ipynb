{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire: Deliverable\n",
    "**Deliverable: wrangle.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve and understand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/student_grades.csv\")\n",
    "# Making a list of missing value types\n",
    "missing_values = [\"n/a\", \"na\", \"--\", \" \"]\n",
    "df = pd.read_csv(\"property data.csv\", na_values = missing_values)\n",
    "\n",
    "df.head()\n",
    "df.shape\n",
    "df.describe()\n",
    "df.info()\n",
    "\n",
    "print(df.isnull().sum()) # find null\n",
    "print(df.columns[df.isnull().any()])\n",
    "df.exam3.value_counts(sort=True, ascending=True)\n",
    "df.replace(r'^\\s*$', np.nan, regex=True, inplace=True) # replace empty with null\n",
    "df = df.dropna().astype('int')\n",
    "df = df.fillna() # fill missing values with a value instead of dropping the rows. \n",
    "# or use imputation to take mean or something to fill in instead of dropping any value or row\n",
    "# ex: fill in mean from same row, model prediction to fill in predicted value\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Distribution\n",
    "**Histograms &/or boxplots** \n",
    "see distribution, skewness, outliers, unit scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 3))\n",
    "\n",
    "for i, col in enumerate(['exam1', 'exam2', 'exam3', 'final_grade']):  \n",
    "    plot_number = i + 1 # i starts at 0, but plot nos should start at 1\n",
    "    series = df[col]  \n",
    "    plt.subplot(1,4, plot_number)\n",
    "    plt.title(col)\n",
    "    series.hist(bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn.boxplot default to plot all numeric variables if we don't specify specific x and y values.\n",
    "# specify the columns to be dismissed\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.boxplot(data=df.drop(columns=['student_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_grades():\n",
    "    grades = pd.read_csv(\"data/student_grades.csv\")\n",
    "    grades.drop(columns='student_id', inplace=True)\n",
    "    grades.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "    df = grades.dropna().astype('int')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Split and Scale \n",
    "**Deliverable: preprocess.py or split_scale.py??**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create training set & testing set\n",
    "\n",
    "- random sampling\n",
    "\n",
    "- split by % (ex: train:75% & test: 25%)\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "- Create a scaled version of attributes and target so that we can compare the importance of each feature.\n",
    "\n",
    "- Assure equal weight \n",
    "\n",
    ">e.g. age and weight\n",
    "\n",
    ">weight would have more impace on a regression model purely because it is in larger units than age if we didn't scaled those<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import wrangle\n",
    "import env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import cleaned data\n",
    "**Deliverable: split_scale.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire data and remove null values \n",
    "df = wrangle.wrangle_grades()\n",
    "\n",
    "# verify acquisition\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Train Test Split\n",
    "# set random seed so the randomization is reproducible\n",
    "train, test = train_test_split(df, train_size = .80, random_state = 123)\n",
    "print(train.shape); print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale data: \n",
    "e.g. avoid particular attributes diluting out the importance of other attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **normalize the numeric range of the attributes**\n",
    "\n",
    "- aka data normalization or standardization (if scaled to a mean of 0 and unit variance)\n",
    "\n",
    "- performed between initial exploration and feature engineering.\n",
    "\n",
    "- each attribute is scaled indpendently, e.g. the mean of exam 2 will not affect how exam 1 is scaled.\n",
    "\n",
    "- thus, it is OKAY to not scale every attributes\n",
    "\n",
    "- helps to identify relationships such as correlations, while exploring\n",
    "\n",
    "#### create the scaler object sklearn.preprocessing.StandardScaler(), \n",
    "#### e.g. fit the object to train scaler.fit(train)\n",
    "#### transform the data in train/test using the parameters estimated from scaler.fit(train): scaler.transform(train), scaler.transform(test)\n",
    "#### to inverse the transformed data back to its original values scaler.inverse_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **StandardScalr** Standard Normal Distribution (mean=0, stdev=1)\n",
    "> linear transformer \n",
    ">\n",
    "> For dev successful/ effective regression model, SVM, clustering algorithms \n",
    ">\n",
    "> individual features need to resemble standard normally distributed data\n",
    "\n",
    "- **QuantileTransformer (uniform)**\n",
    "\n",
    "> **Distort correlations and distances within and across features**\n",
    ">\n",
    "> non-linear transformer \n",
    ">\n",
    "> i.e. values are not the result of a linear function\n",
    ">\n",
    "> smooths out unusual distributions\n",
    ">\n",
    "> spreads out the most frequent values\n",
    ">\n",
    ">reduces the impact of (marginal) outliers \n",
    "\n",
    "\n",
    "- **PowerTransformer**\n",
    "\n",
    ">**Gaussian Scaler** Scale to Gaussian-like distribution\n",
    ">\n",
    "> Use Box-Cox or Yeo-Johnson method to transform to resemble normal or standard normal distrubtion.\n",
    ">\n",
    "> default = zero-mean & unit-variance (standard normal).\n",
    ">\n",
    ">**Yeo-Johnson** supports both positive or negative data\n",
    ">\n",
    "> **Box-Cox** only supports positive data\n",
    "\n",
    "- **RobustScaler**\n",
    "> **Handle outliers**\n",
    ">\n",
    "> use mean and variance $\\neq$ working \n",
    "> median is removed (instead of mean) \n",
    "> \n",
    "> **data is scaled according to a quantile range (the IQR is default)**\n",
    "\n",
    "- **MinMaxScaler**\n",
    "> linear transformation, coz derived from a linear function \n",
    "> \n",
    ">**scale to a range**, result between the given range\n",
    "\n",
    "The values for mean and variance that were computed from the training data in .fit() are stored with the scaler object, so that it can be used when scaling new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, PowerTransformer, RobustScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lecture note\n",
    "pd.cut # create bin out of continuous data -> convert to discrete\n",
    "\n",
    "if dataset big, have to filter, join etc in SQL before pull to local\n",
    "\n",
    "a variable with more than 1 category (discrete) - need feature engineering\n",
    "\n",
    "Y = b0 + b1X (1st degree polynomial)\n",
    "b0: Y-intersect\n",
    "b0, b1: parameters, required to find Y\n",
    "polynomial for none-linear\n",
    "\n",
    ">univatiate (1 feature 1 target) \n",
    ">\n",
    ">multivariate(2+ features, 1 target, y = b1x1 + b2x2)\n",
    ">\n",
    ">poly (y = b0 + b1x^2)???\n",
    "\n",
    "error term -  from real data point to predicted line (for linear regression)\n",
    "\n",
    "Goal, minimize sum sq error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Workflow: make it - fit it - use it**\n",
    ">lr = LinearRegression()\n",
    ">\n",
    ">lr.fit(x,y)\n",
    ">\n",
    ">lr.predict(x)\n",
    "\n",
    "**Feature Engineering, Scaling a variable**\n",
    ">ms = minmaxscaler()\n",
    ">\n",
    ">ms.fit()\n",
    ">\n",
    ">ms.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data integrity, reproducibility\n",
    "when imported into another file (ex: explore.ipynb or report.ipynb), can be used in different places\n",
    "encapsulate the data acquisition sop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clone where my partner left off\n",
    "\n",
    "git clone + paste the url from github \n",
    "\n",
    "essentially make a local copy of the files contributed from my partners\n",
    "\n",
    "cp ../statistics-exercises/env.py ./ # copy from parent to here\n",
    "\n",
    "git checkout file_name.py # go back in time...\n",
    "\n",
    "after a day of work, create a readme document stuff to pay attention ex: remind they need to add their own env.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
