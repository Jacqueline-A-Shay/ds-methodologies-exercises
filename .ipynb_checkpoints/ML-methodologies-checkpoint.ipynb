{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ML Pipeline - demonstrated in simple linear regression__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire Data\n",
    "**Deliverable: wrangle.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve and understand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/student_grades.csv\")\n",
    "# Making a list of missing value types\n",
    "missing_values = [\"n/a\", \"na\", \"--\", \" \"]\n",
    "df = pd.read_csv(\"property data.csv\", na_values = missing_values)\n",
    "\n",
    "df.head()\n",
    "df.shape\n",
    "df.describe()\n",
    "df.info()\n",
    "\n",
    "print(df.isnull().sum()) # find null\n",
    "print(df.columns[df.isnull().any()])\n",
    "df.exam3.value_counts(sort=True, ascending=True)\n",
    "df.replace(r'^\\s*$', np.nan, regex=True, inplace=True) # replace empty with null\n",
    "df = df.dropna().astype('int')\n",
    "df = df.fillna() # fill missing values with a value instead of dropping the rows. \n",
    "# or use imputation to take mean or something to fill in instead of dropping any value or row\n",
    "# ex: fill in mean from same row, model prediction to fill in predicted value\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize all data\n",
    "def peek_data(df: pd.DataFrame):\n",
    "    print('- Shape')\n",
    "    print(df.shape)\n",
    "    print('- Head and Tail')\n",
    "    print(pd.concat([df.head(), df.tail()]))\n",
    "    print('- Numeric Vars')\n",
    "    print(df.describe())\n",
    "    print('- String Columns')\n",
    "    for col in df.select_dtypes('object'):\n",
    "        print('--- {}'.format(col))\n",
    "        print(df[col].value_counts().head())\n",
    "\n",
    "peek_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Distribution\n",
    "**Histograms &/or boxplots** \n",
    "see distribution, skewness, outliers, unit scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 3))\n",
    "\n",
    "for i, col in enumerate(['exam1', 'exam2', 'exam3', 'final_grade']):  \n",
    "    plot_number = i + 1 # i starts at 0, but plot nos should start at 1\n",
    "    series = df[col]  \n",
    "    plt.subplot(1,4, plot_number)\n",
    "    plt.title(col)\n",
    "    series.hist(bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn.boxplot default to plot all numeric variables if we don't specify specific x and y values.\n",
    "# specify the columns to be dismissed\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.boxplot(data=df.drop(columns=['student_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "**Deliverable: explore.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view relationship between 1 var & target\n",
    "sns.jointplot(\"exam1\", \"final_grade\", data=train, kind='reg', height=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn.pairgrid + matplotlib.pyplot.hist + matplotlib.pyplot.scatter\n",
    "# greater flexibility to customize the type of the plots in each position.\n",
    "\n",
    "g = sns.PairGrid(train)\n",
    "g.map_diag(plt.hist)\n",
    "g.map_offdiag(plt.scatter)\n",
    "\n",
    "# heatmap to show correlation\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(train.corr(), cmap='Blues', annot=True)\n",
    "plt.ylim(0, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Split and Scale \n",
    "**Deliverable: split_scale.py**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create training set & testing set\n",
    "\n",
    "- random sampling\n",
    "\n",
    "- split by % (ex: train:75% & test: 25%)\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "- Create a scaled version of attributes and target so that we can compare the importance of each feature.\n",
    "\n",
    "- Assure equal weight \n",
    "\n",
    ">e.g. age and weight\n",
    "\n",
    ">weight would have more impace on a regression model purely because it is in larger units than age if we didn't scaled those<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import wrangle\n",
    "import env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import cleaned data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire data and remove null values \n",
    "df = wrangle.wrangle_grades()\n",
    "\n",
    "# verify acquisition\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Train Test Split\n",
    "# set random seed so the randomization is reproducible\n",
    "train, test = train_test_split(df, train_size = .80, random_state = 123)\n",
    "print(train.shape); print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale data: \n",
    "e.g. avoid particular attributes diluting out the importance of other attributes\n",
    "\n",
    "use scatter plot to present before and after scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **normalize the numeric range of the attributes**\n",
    "\n",
    "- aka data normalization or standardization (if scaled to a mean of 0 and unit variance)\n",
    "\n",
    "- performed between initial exploration and feature engineering.\n",
    "\n",
    "- each attribute is scaled indpendently, e.g. the mean of exam 2 will not affect how exam 1 is scaled.\n",
    "\n",
    "- thus, it is OKAY TO NOT SCALE ALL ATTRIBUTES\n",
    "\n",
    "- helps to identify relationships such as correlations, while exploring\n",
    "\n",
    "> create the scaler object: scaler\n",
    ">\n",
    "> fit: scaler.fit(train)\n",
    ">\n",
    "> train_scaled = scaler.transform(train) & test_scaled = scaler.transform(test)\n",
    ">\n",
    "> inverse the transformed data back to its original values scaler.inverse_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method selection - val of outlier, (sqeeuze based on ratio?)**\n",
    "- **StandardScalr** Standard Normal Distribution (mean=0, stdev=1), result = z-score\n",
    "> linear transformer \n",
    ">\n",
    "> For dev successful/ effective regression model, SVM, clustering algorithms \n",
    ">\n",
    "> individual features need to resemble standard normally distributed data\n",
    "\n",
    "- **QuantileTransformer (uniform)**\n",
    "\n",
    "> **Distort correlations and distances within and across features**\n",
    ">\n",
    "> non-linear transformer \n",
    ">\n",
    "> i.e. values are not the result of a linear function\n",
    ">\n",
    "> smooths out unusual distributions\n",
    ">\n",
    "> spreads out the most frequent values\n",
    ">\n",
    "> reduces the impact of (marginal) outliers \n",
    ">\n",
    "> #sklearn exclusive, pandas is inclusive\n",
    "\n",
    "\n",
    "- **PowerTransformer**\n",
    "\n",
    ">**Gaussian Scaler** Scale to Gaussian-like distribution\n",
    ">\n",
    "> Use Box-Cox or Yeo-Johnson method to transform to resemble normal or standard normal distrubtion.\n",
    ">\n",
    "> default = zero-mean & unit-variance (standard normal).\n",
    ">\n",
    ">**Yeo-Johnson** supports both positive or negative data\n",
    ">\n",
    "> **Box-Cox** only supports positive data\n",
    "\n",
    "- **RobustScaler**\n",
    "> **Handle outliers**\n",
    ">\n",
    "> use mean and variance $\\neq$ working \n",
    "> median is removed (instead of mean) \n",
    "> \n",
    "> **data is scaled according to a quantile range (the IQR is default)**\n",
    "\n",
    "- **MinMaxScaler**\n",
    "> linear transformation, coz derived from a linear function\n",
    ">\n",
    "> sensitive to outlier\n",
    "> \n",
    ">**scale to a range**, result between the given range\n",
    "\n",
    "\n",
    "The values for mean and variance that were computed from the training data in .fit() are stored with the scaler object, so that it can be used when scaling new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, PowerTransformer, RobustScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Workflow: make it - fit it - use it**\n",
    ">lr = LinearRegression()\n",
    ">\n",
    ">lr.fit(x,y)\n",
    ">\n",
    ">lr.predict(x)\n",
    "\n",
    "**Feature Engineering, Scaling a variable**\n",
    ">ms = minmaxscaler()\n",
    ">\n",
    ">ms.fit()\n",
    ">\n",
    ">ms.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Models & Evaluation Techniques\n",
    "foundation for evaluating the effectiveness of a regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (observed value - the estimated value) \n",
    "- vertical distance from the original data point to the expected data point (x_observed = x_expected, y on regression line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SUM OF SQUARED ERRORS (SSE OR RSS, RESIDUAL SUM OF SQUARES)\n",
    "\n",
    "Target: outliers matter\n",
    "How: SUM(Square each residual/error)\n",
    "\n",
    "$SSE = \\sum_{i = 1}^{n} (\\hat{y} - yi)^2$\n",
    "\n",
    "residual plot should be very random - meaning there's no bias in prediction\n",
    "\n",
    "MSE = SSE / n \n",
    "\n",
    "RMSE = sqrt(MSE) # in summary/ average, our prediction is this far away from actual values\n",
    "\n",
    "\n",
    "RSS residual sum of squre = SSE\n",
    "ESS explained SS\n",
    "SSE + ESS = TSS (total)\n",
    "\n",
    "compare with baseline ($mean$)\n",
    "\n",
    "82% of my final grade is explained(model-able) by exam 1 (82% is good num)\n",
    "\n",
    "p-value very close to 0.05, and CI very wide (need large net to catch all the val), we might re-think about including the particular feature in the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep\n",
    "df.head()\n",
    "df.describe()\n",
    "df.info\n",
    "\n",
    "df.exam3.values_counts(ascending = False) # super helpful for busting weird, none numeric values\n",
    "df.replace(r'^\\d+$', np.nan, regex = True, inplace = True)\n",
    "df = df.dropna.astype('int')\n",
    "# outlier\n",
    "\n",
    "# Distribution\n",
    "vis\n",
    "\n",
    "plt.figure(figsize = ( , ))\n",
    "plt.hist(data = df.col_name, bin =     )\n",
    "\n",
    "sns.boxplot(data = df)\n",
    "\n",
    "# split - scale\n",
    "split train & test (not needing to go x, y yet)\n",
    "after scale, if do scatter plot on before scale vs after scale, will see the linear vs non-linear\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols ('y~x') model y using x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "\"Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.\" Jason Brownlee, Machine Learning Mastery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use domain knowledge > construct ad hoc features.\n",
    "> ad hoc \n",
    ">\n",
    "> signifies a solution designed for a specific problem or task, non-generalizable, and not intended to be able to be adapted to other purposes \n",
    "\n",
    "\n",
    "- Perform baseline evaluation to determine if you even need a feature.\n",
    "\n",
    "- feature selection & sample handling\n",
    "\n",
    "\n",
    ">**variable ranking method** Assess features individually \n",
    ">> disect each feature & their impact on system\n",
    ">>\n",
    ">> filter out subset of features if too many or too messy/ noisy\n",
    ">>\n",
    ">> combine interdependent features?\n",
    ">>\n",
    ">> Scale features if not of similar proportion or units\n",
    ">>\n",
    ">> Decision about outliers, discard? ignore? replace?\n",
    "\n",
    "\n",
    "\n",
    ">>**recursive feature elimination** \n",
    ">> recursively remove attributes to meet the number of required features \n",
    ">>\n",
    ">> builds a model w/ remaining attributes\n",
    ">>\n",
    ">> evaluate model performance\n",
    "\n",
    ">>**backward elimination** \n",
    ">> recursively remove the worst performing features one by one till the overall performance of the model comes in acceptable range.\n",
    "\n",
    ">>**Forward selection**\n",
    ">> begins w/ empty equation\n",
    ">>\n",
    ">> Study correlation: **predictors-independent var** vs **target-dependent**\n",
    ">>\n",
    ">> add var to model, begin with theoretically most correlated\n",
    ">>\n",
    ">> evaluate after addition of var\n",
    "\n",
    ">>**Compare results from above methods**\n",
    "\n",
    "- subsample data and re-run analysis \n",
    "> improve performance & understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split (train & test) - Scale - Split (X & Y)\n",
    "\n",
    "\n",
    "- Filter\n",
    "> keep highest correlated attributes\n",
    ">\n",
    "> drop if 1+ var highly correlated\n",
    ">\n",
    "> keep subset of relevant features\n",
    ">\n",
    "> **Pearson pairwise correlation test**\n",
    "> \n",
    "> if the correlation between a pair of features is above a given threshold, you'd remove the one that has larger mean absolute correlation with other features\n",
    ">\n",
    "> **SelectKBest** - removes all but the highest scoring features\n",
    ">> Score: test statistic for the chosen test (ex: chi-squared)\n",
    "\n",
    "- Wrapper (iterative & computationally expensive process, more acurate)\n",
    "> feed features to selected ML algorithm \n",
    ">\n",
    "> add/ remove features based on model performance\n",
    ">> Backward Elimination\n",
    ">>\n",
    ">> Forward Selection\n",
    ">>\n",
    ">> Bidirectional Elimination \n",
    ">>\n",
    ">> Recursive Feature Elimination\n",
    "\n",
    "- Embedded: With each iteration of model training, extract important features\n",
    "> Regularization methods, most common\n",
    "> penalize a feature given a coefficient threshold.\n",
    ">> EX: Lasso regularization. \n",
    "```python\n",
    "if feature == irrelevant(determined by given threshhold):\n",
    "    lasso_penalize = True\n",
    "    feature_coefficient == 0\n",
    "    return irrelevant_feature_removal\n",
    "```\n",
    ">> Other regularization algorithms: \n",
    ">>\n",
    ">> Elastic Net\n",
    ">>\n",
    ">> Ridge Regression\n",
    ">>\n",
    ">> Regularized Regression.\n",
    "\n",
    "- Linear Dimensionality Reduction, Principal component analysis (PCA)\n",
    "**Unsupervised algorithm**\n",
    "\n",
    "> Creates linear combinations of the original features\n",
    ">\n",
    "> new features = orthogonal = uncorrelated\n",
    ">\n",
    "> Rank method: use **explained variance**\n",
    ">\n",
    "> **PC1** explains the most variance in your dataset\n",
    ">\n",
    "> **PC2** explains the second-most variance\n",
    ">\n",
    "> GOAL: keep only min #(principal components) > reach cumulative explained variance of 90%\n",
    ">\n",
    "> $\\because  $ transformation is dependent on scale\n",
    ">\n",
    "> $\\therefore $ **ALWAYS NORMALIZE YOUR DATASET BEFORE PERFORMING PCA**\n",
    ">\n",
    "> Different PCA pkg, (e.g. kernel PCA, sparse PCA, etc.) - each solve unique obstacle\n",
    ">\n",
    "> The new principal components or features are not interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f-regression**\n",
    "\n",
    "F statistic \n",
    "result from ANOVA test or regression analysis (compare baseline & model) \n",
    "if the means between two populations are significantly different\n",
    "\n",
    "Similar to a T statistic from a T-Test\n",
    "\n",
    "A T-test will tell you if a single variable is statistically significant.\n",
    "An F-test will tell you if a group of variables are jointly significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import env\n",
    "import wrangle\n",
    "import split_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESEE, SPLIT, SCALE\n",
    "\n",
    "# acquire data and remove null values \n",
    "df = wrangle.wrangle_grades()\n",
    "\n",
    "# split into train and test\n",
    "train, test = split_scale.split_my_data(df)\n",
    "\n",
    "# scale data using standard scaler\n",
    "scaler, train, test = split_scale.standard_scaler(train, test)\n",
    "\n",
    "# to return to original values\n",
    "# scaler, train, test = scaling.my_inv_transform(scaler, train, test)\n",
    "\n",
    "X_train = train.drop(columns='final_grade')\n",
    "y_train = train[['final_grade']]\n",
    "X_test = test.drop(columns='final_grade')\n",
    "y_test = test[['final_grade']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILTER ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER FEATURE \n",
    "#Using Pearson Correlation\n",
    "plt.figure(figsize=(6,5))\n",
    "cor = train.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View only the correlations of each attribute with the target variable, \n",
    "# filter down to only those above a certain value\n",
    "\n",
    "#Correlation with output variable\n",
    "cor_target = abs(cor[\"final_grade\"])\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.5]\n",
    "relevant_features\n",
    "\n",
    "# output:\n",
    "#\n",
    "# exam1          0.984101\n",
    "# exam2          0.922598\n",
    "# exam3          0.950309\n",
    "# final_grade    1.000000\n",
    "# Name: final_grade, dtype: float64\n",
    "\n",
    "# may decide to only use one exam (exam1 most correlated) \n",
    "# or \n",
    "# create new features, such as exam2_delta = exam2 - exam1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use test statistics to score & select feature\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Chi\n",
    "# all value need to be > 0\n",
    "# might need to scale all value so all > 0\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "scaler, train_unscaled, test_unscaled = split_scale.my_inv_transform(scaler, train_scaled=train, test_scaled=test)\n",
    "\n",
    "X_train_unscaled = train_unscaled.drop(columns='final_grade')\n",
    "y_train_unscaled = train_unscaled[['final_grade']]\n",
    "\n",
    "chi_selector = SelectKBest(chi2, k=2)\n",
    "\n",
    "chi_selector.fit(X_train_unscaled, y_train_unscaled)\n",
    "\n",
    "chi_support = chi_selector.get_support()\n",
    "chi_feature = X_train_unscaled.loc[:,chi_support].columns.tolist()\n",
    "\n",
    "print(str(len(chi_feature)), 'selected features')\n",
    "print(chi_feature)\n",
    "\n",
    "# OUTPUT: Selected features, in a list\n",
    "# 2 selected features\n",
    "# ['exam1', 'exam2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using f-regression\n",
    "# F statistic \n",
    "# result from ANOVA test or regression analysis (compare baseline & model) \n",
    "# if the means between two populations are significantly different\n",
    "\n",
    "# Similar to a T statistic from a T-Test\n",
    "\n",
    "# A T-test will tell you if a single variable is statistically significant.\n",
    "# An F-test will tell you if a group of variables are jointly significant.\n",
    "\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "f_selector = SelectKBest(f_regression, k=2)\n",
    "f_selector.fit(X_train,y_train)\n",
    "f_support = f_selector.get_support()\n",
    "\n",
    "f_feature = X_train.loc[:,f_support].columns.tolist()\n",
    "print(str(len(f_feature)), 'selected features')\n",
    "print(f_feature)\n",
    "\n",
    "# OUTPUT: Selected features, in a list\n",
    "# 2 selected features\n",
    "# ['exam1', 'exam3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRAPPER ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward Elimination using OLS**\n",
    "\n",
    "- We check the performance of the model \n",
    "\n",
    "- iteratively remove the worst performing features one by one till the overall performance of the model comes in acceptable range.\n",
    "\n",
    "- Performance metric/ measurement pre-determined\n",
    "> p-value used for demo\n",
    "```python\n",
    "if p > 0.05: \n",
    "    remove feature\n",
    "else:  \n",
    "    keep feature\n",
    "```    \n",
    "run evaluation in a loop, return final set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# create the OLS object:\n",
    "ols_model = sm.OLS(y_train, X_train)\n",
    "\n",
    "# fit the model:\n",
    "fit = ols_model.fit()\n",
    "\n",
    "# summarize:\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(X_train.columns)\n",
    "pmax = 1\n",
    "while (len(cols)>0):\n",
    "    p= []\n",
    "    X_1 = X_train[cols]\n",
    "    X_1 = sm.add_constant(X_1)\n",
    "    model = sm.OLS(y_train,X_1).fit()\n",
    "    p = pd.Series(model.pvalues.values[1:],index = cols)\n",
    "    pmax = max(p)\n",
    "    feature_with_p_max = p.idxmax()\n",
    "    if(pmax>0.05):\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "selected_features_BE = cols\n",
    "print(selected_features_BE) #output as list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recursive Feature Elimination (RFE)**\n",
    "\n",
    "Recursively removes attributes and then builds a model on those attributes that remain.\n",
    "\n",
    "It uses accuracy metric to rank the feature according to their importance.\n",
    "\n",
    "**INPUT:**\n",
    "model to be used \n",
    "number of required features\n",
    "\n",
    "**RETURN**\n",
    "ranking of all the variables\n",
    "- 1: most important\n",
    "- True = relevant feature\n",
    "- False = irrelevant feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "#Initializing RFE model, with parameter to select top 2 features. \n",
    "rfe = RFE(model, 2)\n",
    "\n",
    "#Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(X_train,y_train)  \n",
    "\n",
    "#Fitting the data to model\n",
    "model.fit(X_rfe,y_train)\n",
    "\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features_list=np.arange(1,3)\n",
    "high_score=0\n",
    "\n",
    "#Variable to store the optimum features\n",
    "number_of_features=0           \n",
    "score_list =[]\n",
    "\n",
    "for n in range(len(number_of_features_list)):\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model,number_of_features_list[n])\n",
    "    X_train_rfe = rfe.fit_transform(X_train,y_train)\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    model.fit(X_train_rfe,y_train)\n",
    "    score = model.score(X_test_rfe,y_test)\n",
    "    score_list.append(score)\n",
    "    if(score>high_score):\n",
    "        high_score = score\n",
    "        number_of_features = number_of_features_list[n]\n",
    "\n",
    "print(\"Optimum number of features: %d\" %number_of_features)\n",
    "print(\"Score with %d features: %f\" % (number_of_features, high_score))\n",
    "\n",
    "# output \n",
    "# Optimum number of features: 2\n",
    "# Score with 2 features: 0.965926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(X_train.columns)\n",
    "model = LinearRegression()\n",
    "\n",
    "#Initializing RFE model\n",
    "rfe = RFE(model, 2)\n",
    "\n",
    "#Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(X_train,y_train)  \n",
    "\n",
    "#Fitting the data to model\n",
    "model.fit(X_rfe,y_train)\n",
    "temp = pd.Series(rfe.support_,index = cols)\n",
    "selected_features_rfe = temp[temp==True].index\n",
    "\n",
    "print(selected_features_rfe)\n",
    "\n",
    "# output\n",
    "# Index(['exam1', 'exam3'], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedded\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "reg = LassoCV()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\n",
    "print(\"Best score using built-in LassoCV: %f\" %reg.score(X_train,y_train))\n",
    "coef = pd.Series(reg.coef_, index = X_train.columns)\n",
    "\n",
    "\n",
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
    "\n",
    "# output\n",
    "# Best alpha using built-in LassoCV: 0.001301\n",
    "# Best score using built-in LassoCV: 0.973583\n",
    "# Lasso picked 3 variables and eliminated the other 0 variables\n",
    "\n",
    "# higher Î±\n",
    "# the fewer the features have non-zero values\n",
    "# aka Lasso accepted more features\n",
    "\n",
    "imp_coef = coef.sort_values()\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (4.0, 5.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Feature importance using Lasso Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=1, copy=True, whiten=False, svd_solver='auto', random_state=123)\n",
    "pca.fit(X_train)\n",
    "X = pca.transform(X_train)\n",
    "print(pca.n_components_)\n",
    "print(len(X))\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## NOTE FOR EXERCISE CODE ############\n",
    "\n",
    "# use test statistics to score & select feature\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Chi\n",
    "# all value need to be > 0\n",
    "# might need to scale all value so all > 0\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "scaler, train_unscaled, test_unscaled = \n",
    "split_scale.my_inv_transform(scaler, train_scaled=train, test_scaled=test)\n",
    "\n",
    "X_train_unscaled = train_unscaled.drop(columns='final_grade')\n",
    "y_train_unscaled = train_unscaled[['final_grade']]\n",
    "\n",
    "chi_selector = SelectKBest(chi2, k=2)\n",
    "\n",
    "chi_selector.fit(X_train_unscaled, y_train_unscaled)\n",
    "\n",
    "chi_support = chi_selector.get_support()\n",
    "chi_feature = X_train_unscaled.loc[:,chi_support].columns.tolist()\n",
    "\n",
    "print(str(len(chi_feature)), 'selected features')\n",
    "print(chi_feature)\n",
    "\n",
    "# OUTPUT: Selected features, in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_kbest_freg(X_train, y_train (scaled), k): #no1 unscaled #no2 scaled()\n",
    "    from sklearn.feature_selection import SelectKBest\n",
    "    from sklearn.feature_selection import f_regression\n",
    "\n",
    "    \n",
    "    return a list of the top k features\n",
    "\n",
    "# 2 selected features\n",
    "# ['exam1', 'exam3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## NOTE FOR EXERCISE CODE ##############\n",
    "# Using f-regression\n",
    "# F statistic \n",
    "# result from ANOVA test or regression analysis (compare baseline & model) \n",
    "# if the means between two populations are significantly different\n",
    "\n",
    "# Similar to a T statistic from a T-Test\n",
    "\n",
    "# A T-test will tell you if a single variable is statistically significant.\n",
    "# An F-test will tell you if a group of variables are jointly significant.\n",
    "\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "f_selector = SelectKBest(f_regression, k=2)\n",
    "f_selector.fit(X_train,y_train)\n",
    "f_support = f_selector.get_support()\n",
    "\n",
    "f_feature = X_train.loc[:,f_support].columns.tolist()\n",
    "print(str(len(f_feature)), 'selected features')\n",
    "print(f_feature)\n",
    "\n",
    "# OUTPUT: Selected features, in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- heatmap, correlation\n",
    "- construct new feature, if 2 var very similar\n",
    "> NEW = df.exam1 - df.exam2 # constructed new, will be at different scale than original data\n",
    "> thus definitely need scale\n",
    "\n",
    "- split-scale\n",
    "\n",
    "- corr.threshold\n",
    "- selectkbest, based on a choosen stats, ex: f-regression which is for univariate\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "X_train = scaled.drop(columns = ['final_grade'])\n",
    "y_train = scaled[[\"final_grade\"]]\n",
    "\n",
    "f_selector = SelectKBest(f-regression, k = 3).fit(train)\n",
    "f_support = f_selector.get_support()\n",
    "\n",
    "print(X_train.loc[:, f_support].columns.tolist()) # pull the order in original data\n",
    "f_selector.scores_ # sho score for each var\n",
    "\n",
    "cols = list(X_train.columns) # get list of column names\n",
    "\n",
    "```\n",
    "\n",
    "- backward, use OLS, take all < 0.05, identify, \n",
    "- RFE model + recurrsively remove\n",
    "\n",
    "- embedded, make zero instead of drop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter - computational limitation when dataset large\n",
    "> only evaluate\n",
    "> correlation & selectkbest\n",
    "\n",
    "wrapper - performance of model to decide feature\n",
    ">> **backward** \n",
    ">>\n",
    ">> run model, check (ex: use p-value), remove the worst\n",
    ">>\n",
    "```python\n",
    "if pmax>0.05:\n",
    "    remove\n",
    "   ```\n",
    ">>\n",
    ">> **recursive feature elimination**\n",
    ">>\n",
    ">> take the model and number of features provided\n",
    ">>\n",
    ">> return ranking of all variables and support (T/F)\n",
    "\n",
    "```python\n",
    "reg = LinearRegression()\n",
    "rfe = RFE(reg, 3)\n",
    "X_rfe = rfe.fit_transform(X_train, y_train)\n",
    "```\n",
    "\n",
    "embedded - adapt, not discard variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**\n",
    "- supervised machine learning \n",
    ">> \n",
    ">> Relationship between one (univariate) or more (multivariate) features \n",
    ">>\n",
    ">> how feature(s) contribute to 1 particular outcome - **continuous target variable**\n",
    "\n",
    "1. Find an algorithm that takes in a set of parameters and returns a predicted data set\n",
    "2. Identify the 'error function' to report difference between data & model prediction \n",
    "3. Find the parameters that minimize this difference.\n",
    "\n",
    "regression: find **line or plane** that minimizes the errors in our predictions when compared to the labeled data\n",
    "\n",
    "Univariate: $y = b0 + b1x$\n",
    "\n",
    "Polynomial: $y = b0 + b1x^2 $\n",
    "\n",
    "Mutivariate: $y = b0 + b1x + b2x2 + ... + bnxn$ \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "from math import sqrt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import env\n",
    "import wrangle\n",
    "import split_scale\n",
    "import features\n",
    "\n",
    "# acquire data and remove null values \n",
    "df = wrangle.wrangle_grades()\n",
    "\n",
    "# split into train and test\n",
    "train, test = split_scale.split_my_data(df)\n",
    "\n",
    "# scale data using standard scaler\n",
    "# use std scaler to scale train & test, store into scaler\n",
    "scaler, train, test = split_scale.standard_scaler(train, test)\n",
    "\n",
    "# to return to original values\n",
    "# scaler, train, test = scaling.my_inv_transform(scaler, train, test)\n",
    "\n",
    "# assign variable vs target\n",
    "X_train = train.drop(columns='final_grade')\n",
    "y_train = train[['final_grade']]\n",
    "X_test = test.drop(columns='final_grade')\n",
    "y_test = test[['final_grade']]\n",
    "\n",
    "# Perform feature selection using RFE\n",
    "number_of_features = features.optimal_number_of_features(X_train, y_train, \n",
    "                                        X_test, y_test)\n",
    "\n",
    "selected_features = features.optimal_features(X_train, y_train, number_of_features)\n",
    "\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm1 = LinearRegression()\n",
    "\n",
    "print(lm1)\n",
    "\n",
    "# OUTPUT:\n",
    "# LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
    "\n",
    "\n",
    "lm1.fit(X_train, y_train)\n",
    "print(\"Linear Model:\", lm1)\n",
    "\n",
    "lm1_y_intercept = lm1.intercept_\n",
    "print(\"intercept: \", lm1_y_intercept)\n",
    "\n",
    "lm1_coefficients = lm1.coef_\n",
    "print(\"coefficients: \", lm1_coefficients)\n",
    "\n",
    "# Output:\n",
    "# Linear Model: LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
    "# intercept:  [2.62068042e-17]\n",
    "# coefficients:  [[0.78604344 0.21040984]]\n",
    "\n",
    "print('{} = b + m1 * {} + m2 * {}'.format(y_train.columns[0], X_train.columns[0],X_train.columns[1]))\n",
    "print('    y-intercept  (b): %.2f' % lm1_y_intercept)\n",
    "print('    coefficient (m1): %.2f' % lm1_coefficients[0][0])\n",
    "print('    coefficient (m2): %.2f' % lm1_coefficients[0][1])\n",
    "\n",
    "# Output:\n",
    "# final_grade = b + m1 * exam1 + m2 * exam3\n",
    "# y-intercept  (b): 0.00\n",
    "# coefficient (m1): 0.79\n",
    "# coefficient (m2): 0.21\n",
    "\n",
    "## In sample prediction\n",
    "y_pred_lm1 = lm1.predict(X_train)\n",
    "\n",
    "## In sample evaluation\n",
    "## use performance metrics: mean squared error & r-squared values\n",
    "\n",
    "mse_lm1 = mean_squared_error(y_train, y_pred_lm1)\n",
    "print(\"linear model\\n  mean squared error: {:.3}\".format(mse_lm1)) \n",
    "\n",
    "r2_lm1 = r2_score(y_train, y_pred_lm1)\n",
    "print('  {:.2%} of the variance in the student''s final grade can be explained by the grades on exam 1 and 3.'.format(r2_lm1))\n",
    "\n",
    "# Output of in sample evaluation\n",
    "# linear model\n",
    "# mean squared error: 0.0265\n",
    "# 97.35% of the variance in the students final grade can be explained by the grades on exam 1 and 3.\n",
    "\n",
    "\n",
    "# Establish baseline\n",
    "from math import sqrt\n",
    "\n",
    "y_pred_baseline = np.array([y_train.mean()[0]]*len(y_train))\n",
    "MSE = mean_squared_error(y_train, y_pred_baseline)\n",
    "SSE = MSE*len(y_train)\n",
    "RMSE = sqrt(MSE)\n",
    "\n",
    "evs = explained_variance_score(y_train, y_pred_baseline)\n",
    "\n",
    "print('sum of squared errors\\n model: {:.5}'.format(SSE))\n",
    "print('  {:.2%} of the variance in the student''s final grade can be explained by the grades on exam 1 and 3.'.format(evs))\n",
    "\n",
    "#output\n",
    "# sum of squared errors\n",
    "# model: 81.0\n",
    "# 0.00% of the variance in the students final grade can be explained by the grades on exam 1 and 3.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Note**\n",
    "\n",
    "- **predict values** of single unit properties (what's single unit? stand alone house?)\n",
    "\n",
    "- used by tax district **last transaction: May, 2017 - June, 2017**\n",
    "\n",
    "Cue\n",
    "\n",
    "- **Property location** Identify thru **county, state\n",
    "> property tax calc at county level\n",
    "\n",
    "- **Distribution of tax rate, county level**\n",
    "> data: home tax amounts & tax value \n",
    "> \n",
    "> Deliverable: distribution of tax rate for each county -> tax vs physicaly mapping\n",
    ">\n",
    "> want to see how much tax vary within the properties in the county and \n",
    "> \n",
    "> the rates the bulk of the properties sit around. \n",
    ">\n",
    "> Mapping is separate from Modeling, **tax is the target for modeling**\n",
    "\n",
    "- 1st model: \n",
    "> taxvaluedollarcnt \n",
    ">\n",
    "> estimate properties assessed value\n",
    ">\n",
    "> use sqft., #bed/ #bath\n",
    ">\n",
    "> may expand from here\n",
    ">\n",
    "> be careful to make sure using the best fields to represent sqft, #bed/ #bath\n",
    ">\n",
    "> best => the most accurate and available information. \n",
    ">\n",
    "> You will need to do some data investigation in the database and use your domain expertise to make some judgement calls.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stakeholder** zillow data science team\n",
    "\n",
    "state your goals as if you were delivering this to zillow. \n",
    "the goals as you understand them and as you have taken and acted upon through your research\n",
    "\n",
    "**Deliverables** \n",
    "\n",
    "1. A report (presentation, both verbal & slides)\n",
    "> summary of findings\n",
    ">\n",
    "> drivers of Zestimate error\n",
    ">\n",
    "> come from analysis from exploration phase of the pipeline \n",
    ">\n",
    "> charts form - driver of errors\n",
    "\n",
    "2. A github repository w/:\n",
    "> jupyter notebook: walks through the pipeline  \n",
    ">\n",
    "> .py files: for model reproduction\n",
    "\n",
    "Pipeline\n",
    "\n",
    "- PROJECT PLANNING & README\n",
    "\n",
    "- Brainstorming ideas, \n",
    "- hypotheses, \n",
    "- how variables might impact or relate to each other, \n",
    "> within independent variables\n",
    ">\n",
    "> between the independent variables and dependent variable, \n",
    "> \n",
    "> new features you may have while first looking at the existing variables \n",
    ">\n",
    "> potential challenge\n",
    "\n",
    "**README.md**\n",
    "- description of project \n",
    "- instructions for reproducibility\n",
    "\n",
    "**ACQUIRE**\n",
    "\n",
    "**acquire.py** \n",
    "- Product: data acquired, along with overview of data\n",
    "- Summary of dataframe \n",
    "> first few rows\n",
    ">\n",
    "> data types\n",
    ">\n",
    "> summary stats\n",
    ">\n",
    "> column names\n",
    ">\n",
    "> shape of the data frame\n",
    ">\n",
    "> etc\n",
    "\n",
    "**PREP**\n",
    "\n",
    "**prep.py**\n",
    "- Product: tidy, ready to be analyzed data.\n",
    "- Dictionary: data exploration, data tidy methods and logic.\n",
    "\n",
    "\n",
    "**SPLIT & SCALE**\n",
    "\n",
    "**split_scale.py**\n",
    "- Product: 2 dataframes - splited data (train & test), scaled data\n",
    "\n",
    "\n",
    "**DATA EXPLORATION** \n",
    "\n",
    "**explore.py**\n",
    "- Product: an analysis report of key takeaways & existing relationships among features \n",
    "\n",
    "Address each of the questions you posed in your planning and brainstorming \n",
    "any others you have come up with along the way through visual or statistical analysis.\n",
    "\n",
    "product\n",
    "the findings from your analysis that will be used in your final report, \n",
    "answers to specific questions your customers has asked, \n",
    "information to move forward toward building a model\n",
    "\n",
    "Run at least 1 t-test and 1 correlation test (but as many as you need!)\n",
    "\n",
    "Visualization: relationships variable-target, variable-variable (heatmap?)\n",
    "> shed light into potential need to discard variable or construct new feature based on 2+ variable\n",
    "\n",
    "\n",
    "**FEATURE SELECTION**\n",
    "\n",
    "**feature_selection.py**\n",
    "\n",
    "- Product: dataframe containing the features selected for model construction. \n",
    "- Dictionary of features and methods\n",
    "\n",
    "\n",
    "**MODELING & EVALUATION**\n",
    "Goal: develop a regression model that performs better than a baseline.\n",
    "\n",
    "You must evaluate a baseline model, and show how the model you end up with performs better than that.\n",
    "\n",
    "model.py: will have the functions to fit, predict and evaluate the model\n",
    "\n",
    "Your notebook will contain various algorithms and/or hyperparameters tried, along with the evaluation code and results, before settling on the final algorithm.\n",
    "\n",
    "Be sure and evaluate your model using the standard techniques: plotting the residuals, computing the evaluation metric (SSE, RMSE, and/or MSE), comparing to baseline, plotting \n",
    "y\n",
    " by \n",
    "^\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lecture note\n",
    "pd.cut # create bin out of continuous data -> convert to discrete\n",
    "\n",
    "if dataset big, have to filter, join etc in SQL before pull to local\n",
    "\n",
    "a variable with more than 1 category (discrete) - need feature engineering\n",
    "\n",
    "Y = b0 + b1X (1st degree polynomial)\n",
    "b0: Y-intersect\n",
    "b0, b1: parameters, required to find Y\n",
    "polynomial for none-linear\n",
    "\n",
    ">univatiate (1 feature 1 target) \n",
    ">\n",
    ">multivariate(2+ features, 1 target, y = b1x1 + b2x2)\n",
    ">\n",
    ">poly (y = b0 + b1x^2)???\n",
    "\n",
    "error term -  from real data point to predicted line (for linear regression)\n",
    "\n",
    "Goal, minimize sum sq error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data integrity, reproducibility\n",
    "when imported into another file (ex: explore.ipynb or report.ipynb), can be used in different places\n",
    "encapsulate the data acquisition sop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clone where my partner left off\n",
    "\n",
    "git clone + paste the url from github \n",
    "\n",
    "essentially make a local copy of the files contributed from my partners\n",
    "\n",
    "cp ../statistics-exercises/env.py ./ # copy from parent to here\n",
    "\n",
    "git checkout file_name.py # go back in time...\n",
    "\n",
    "after a day of work, create a readme document stuff to pay attention ex: remind they need to add their own env.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
